{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import torch \n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_db = '../data/TUDataset'\n",
    "dataset_tud = TUDataset(root=path_db, name='MUTAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_tud[12]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.tu_dataset.TUDataset"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_tud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 50], x=[22, 7], edge_attr=[50, 4], y=[1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data    # obtain dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 0,  5],\n",
       "        [ 1,  0],\n",
       "        [ 1,  2],\n",
       "        [ 2,  1],\n",
       "        [ 2,  3],\n",
       "        [ 3,  2],\n",
       "        [ 3,  4],\n",
       "        [ 3, 12],\n",
       "        [ 4,  3],\n",
       "        [ 4,  5],\n",
       "        [ 4,  6],\n",
       "        [ 5,  0],\n",
       "        [ 5,  4],\n",
       "        [ 6,  4],\n",
       "        [ 6,  7],\n",
       "        [ 6, 11],\n",
       "        [ 7,  6],\n",
       "        [ 7,  8],\n",
       "        [ 8,  7],\n",
       "        [ 8,  9],\n",
       "        [ 9,  8],\n",
       "        [ 9, 10],\n",
       "        [ 9, 19],\n",
       "        [10,  9],\n",
       "        [10, 11],\n",
       "        [10, 15],\n",
       "        [11,  6],\n",
       "        [11, 10],\n",
       "        [11, 12],\n",
       "        [12,  3],\n",
       "        [12, 11],\n",
       "        [12, 13],\n",
       "        [13, 12],\n",
       "        [13, 14],\n",
       "        [14, 13],\n",
       "        [14, 15],\n",
       "        [14, 16],\n",
       "        [15, 10],\n",
       "        [15, 14],\n",
       "        [16, 14],\n",
       "        [16, 17],\n",
       "        [16, 18],\n",
       "        [17, 16],\n",
       "        [18, 16],\n",
       "        [19,  9],\n",
       "        [19, 20],\n",
       "        [19, 21],\n",
       "        [20, 19],\n",
       "        [21, 19]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index.T         # obtain every undirected connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x    # obtain node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_attr       # obtain edges labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = data.x.numpy()  # Assuming node features are stored in 'x'\n",
    "edge_index = data.edge_index.numpy()  # Assuming edge information is in 'edge_index'\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with features\n",
    "for i in range(x.shape[0]):\n",
    "    G.add_node(i, features=x[i])\n",
    "# Add edges\n",
    "for j in range(edge_index.shape[1]):\n",
    "    G.add_edge(edge_index[0, j], edge_index[1, j])\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=800, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCP Data using Simeon preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "\n",
    "def create_fc_matrices(scan, window_size=30, step_size=10):\n",
    "    \"\"\"\n",
    "    Create functional connectivity matrices using a sliding window approach.\n",
    "    \"\"\"\n",
    "    n_timepoints = scan.shape[1]\n",
    "    fc_matrices = []\n",
    "\n",
    "    for start in range(0, n_timepoints - window_size + 1, step_size):\n",
    "        window = scan[:, start:start + window_size]\n",
    "        correlation_matrix = np.corrcoef(window)   # pearson correlation \n",
    "        fc_matrices.append(correlation_matrix)\n",
    "\n",
    "    return fc_matrices\n",
    "\n",
    "\n",
    "def create_fc_matrices_dbgsl(scan, window_size=30, step_size=10):\n",
    "    \"\"\"\n",
    "    Create functional connectivity matrices using a sliding window approach.\n",
    "    \"\"\"\n",
    "    n_timepoints = scan.shape[1]\n",
    "    t_repetition = (n_timepoints - 2*(window_size - 1) - 1)//(step_size + 1)\n",
    "    fc_matrices = []\n",
    "\n",
    "    for t in range(t_repetition):\n",
    "        window = scan[:, t*step_size:t*step_size+window_size]\n",
    "        correlation_matrix = np.corrcoef(window)   # pearson correlation \n",
    "        fc_matrices.append(correlation_matrix)\n",
    "\n",
    "    return fc_matrices\n",
    "\n",
    "\n",
    "def create_ica_fc_matrices(scan, window_size=30, step_size=30, n_components=360):\n",
    "    \"\"\"\n",
    "    Create functional connectivity matrices using ICA on sliding windows.\n",
    "    \"\"\"\n",
    "    n_timepoints = scan.shape[1]\n",
    "    fc_matrices = []\n",
    "\n",
    "    for start in range(0, n_timepoints - window_size + 1, step_size):\n",
    "        window = scan[:, start:start + window_size]\n",
    "        ica = FastICA(n_components=n_components, random_state=42)\n",
    "        ica_components = ica.fit_transform(window)\n",
    "        fc_matrix = np.corrcoef(ica_components)\n",
    "        fc_matrices.append(fc_matrix)\n",
    "\n",
    "    return fc_matrices\n",
    "\n",
    "\n",
    "\n",
    "def threshold_fc_matrix(fc_matrix, percentile=5):\n",
    "    \"\"\"\n",
    "    Threshold the FC matrix to keep only the top percentile connections.\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(fc_matrix[np.tril_indices_from(fc_matrix, k=-1)], 100 - percentile)   \n",
    "    graph = (fc_matrix > threshold).astype(int)\n",
    "    np.fill_diagonal(graph, 0)  # remove self-edges\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_networkx_graph(matrix):\n",
    "    G = nx.Graph(matrix)\n",
    "    return G\n",
    "\n",
    "\n",
    "def convert_to_pyg_graph(nx_graph, label):\n",
    "    graph = nx.Graph(nx_graph)\n",
    "    edges = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(np.identity(graph.number_of_nodes()), dtype=torch.float)\n",
    "    y = torch.tensor([label], dtype=torch.long)\n",
    "    return Data(x=x, edge_index=edges, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE_PATH:  ../data/hcp/raw/100206_0.npy\n",
      "time_series_data SHAPE:  (360, 490)\n",
      "fc_matrices_39 SHAPE:  torch.Size([1, 39, 360, 360])\n",
      "fc_matrices_tn SHAPE:  torch.Size([1, 47, 360, 360])\n"
     ]
    }
   ],
   "source": [
    "# ----------- One raw as example -------------------------\n",
    "\n",
    "data_path = '../data/hcp/raw'\n",
    "file_name = '100206_0.npy'\n",
    "file_path = os.path.join(data_path, file_name)\n",
    "print('FILE_PATH: ', file_path)\n",
    "time_series_data = np.load(file_path)[:, :490]\n",
    "print('time_series_data SHAPE: ', time_series_data.shape)\n",
    "label = int(os.path.basename(file_path).split('_')[-1].split('.')[0])\n",
    "\n",
    "fc_matrices = create_fc_matrices(time_series_data)\n",
    "fc_matrices_dbgsl = create_fc_matrices_dbgsl(time_series_data)\n",
    "fc_matrices_dbgsl_tn = torch.tensor(fc_matrices_dbgsl).reshape(1, len(fc_matrices_dbgsl), 360, 360) # --> new simeon\n",
    "print('fc_matrices_39 SHAPE: ', fc_matrices_dbgsl_tn.shape)\n",
    "\n",
    "fc_matrices_tn = torch.tensor(fc_matrices).reshape(1, len(fc_matrices), 360, 360)\n",
    "print('fc_matrices_tn SHAPE: ', fc_matrices_tn.shape)\n",
    "graphs = [threshold_fc_matrix(fc) for fc in fc_matrices] \n",
    "graph = graphs[0]\n",
    "graph_nx = convert_to_pyg_graph(graph, label)\n",
    "\n",
    "# ----------- One raw as example -------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_repetition:  39\n",
      "RESHAPING time_series_data:  torch.Size([1, 360, 490])\n",
      "SPLITTING WINDOW time_series_data:  torch.Size([1, 39, 360, 30])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1   # remember to change\n",
    "lr = 0.003\n",
    "n_episodes = 1000   # remember to change\n",
    "device = 'cpu'\n",
    "n_itcn_layers = 3\n",
    "n_gru_layers = 1\n",
    "n_neurons = 360\n",
    "tau = 0.1\n",
    "kernel_list = [3, 5, 7]\n",
    "itcn_d = 9\n",
    "ebd_d = 3\n",
    "gcn_d = 5\n",
    "n_classes = 1\n",
    "\n",
    "T = 490\n",
    "len_window = 30   \n",
    "stride = 10  \n",
    "t_repetition = (T - 2*(len_window - 1) - 1)//(stride + 1)\n",
    "print('t_repetition: ', t_repetition)\n",
    "\n",
    "\n",
    "def get_x_split(x: pt.Tensor) -> pt.Tensor:\n",
    "    x_split = pt.stack([x[:, :, t*stride:t*stride+len_window] for t in range(t_repetition)], 1)\n",
    "    return x_split.float()\n",
    "\n",
    "def get_node_features(x_split: pt.Tensor) -> pt.Tensor:\n",
    "    x_split_avg = pt.mean(x_split, -1, keepdim=True)\n",
    "    x_split_std = pt.std(x_split, -1, keepdim=True)\n",
    "    x_split_cov = pt.matmul(x_split - x_split_avg, pt.transpose(x_split - x_split_avg, 2, 3))\n",
    "    node_features = x_split_cov/pt.matmul(x_split_std, pt.transpose(x_split_std, 2, 3))\n",
    "    return node_features\n",
    "\n",
    "def get_coo(adjacency_matrix: pt.Tensor) -> pt.Tensor:\n",
    "    i = 0\n",
    "    edge_indices = pt.nonzero(adjacency_matrix > 0, as_tuple=False).T\n",
    "    edge_index_batch = pt.clone(edge_indices[1:3, :])\n",
    "    for t in range(len(edge_indices[0])):\n",
    "        if i < edge_indices[0][t]:\n",
    "            i = i + 1\n",
    "            n_nodes = max(edge_indices[1][t-1], edge_indices[2][t-1])+1\n",
    "            edge_index_batch[0][t:] = edge_index_batch[0][t:] + n_nodes\n",
    "            edge_index_batch[1][t:] = edge_index_batch[1][t:] + n_nodes\n",
    "    edge_attr_batch = adjacency_matrix[adjacency_matrix > 0].unsqueeze(-1)\n",
    "    batch = edge_indices[0]\n",
    "    return edge_index_batch, edge_attr_batch, batch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def construct_graph(x_ebd: torch.Tensor) -> torch.Tensor:\n",
    "    batch_size = x_ebd.shape[0]\n",
    "    batch_adjacency_matrices = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sample = x_ebd[i]\n",
    "        sample = F.softmax(sample, -1)\n",
    "        adjacency_matrix = torch.matmul(sample, sample.transpose(1, 2))\n",
    "        batch_adjacency_matrices.append(adjacency_matrix)\n",
    "    adjacency_matrix_batch = torch.stack(batch_adjacency_matrices, dim=0)\n",
    "\n",
    "    return adjacency_matrix_batch\n",
    "\n",
    "def sparsify(adjacency_matrix_batch: torch.Tensor) -> torch.Tensor:\n",
    "    threshold = torch.nn.parameter.Parameter(torch.full((1,), -5.0))\n",
    "    batch_sparse_adjacency = []\n",
    "\n",
    "    for i in range(adjacency_matrix_batch.size(0)):\n",
    "        adjacency_matrix = adjacency_matrix_batch[i]\n",
    "        sparse_adjacency = F.relu(adjacency_matrix - torch.sigmoid(threshold))\n",
    "        batch_sparse_adjacency.append(sparse_adjacency)\n",
    "    sparse_adjacency_batch = torch.stack(batch_sparse_adjacency, dim=0)\n",
    "\n",
    "    return sparse_adjacency_batch\n",
    "\n",
    "\n",
    "\n",
    "time_series_torch = torch.from_numpy(time_series_data).reshape(1, time_series_data.shape[0], time_series_data.shape[1])\n",
    "print('RESHAPING time_series_data: ', time_series_torch.shape)\n",
    "\n",
    "x_split = get_x_split(time_series_torch)\n",
    "\n",
    "print('SPLITTING WINDOW time_series_data: ', x_split.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/hcp/raw'\n",
    "dataset_input = []\n",
    "dataset_ground = []\n",
    "for file_name in os.listdir(data_path):\n",
    "    if file_name.endswith(\".npy\"):\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        time_series_data = np.load(file_path)[:, :490]\n",
    "        time_series_torch = torch.from_numpy(time_series_data).reshape(1, \n",
    "                                             time_series_data.shape[0], time_series_data.shape[1])\n",
    "        x_split = get_x_split(time_series_torch)\n",
    "        dataset_input.append(x_split)\n",
    "        fc_matrices = create_fc_matrices_dbgsl(time_series_data)\n",
    "        fc_matrices_tn = torch.tensor(fc_matrices).reshape(1, len(fc_matrices), 360, 360)\n",
    "        ground_truth = fc_matrices_tn.float()\n",
    "        dataset_ground.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 39, 360, 30])\n",
      "torch.Size([300, 39, 360, 360])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.cat(dataset_input, dim=0)\n",
    "ground_truth_tens = torch.cat(dataset_ground, dim=0)\n",
    "print(input_tensor.shape)\n",
    "print(ground_truth_tens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InceptionTC(nn.Module):\n",
    "    def __init__(self, itcn_d, dilation):\n",
    "        super(InceptionTC, self).__init__()\n",
    "        self.t_conv_0 = nn.Conv1d(itcn_d, itcn_d//3, kernel_list[0], dilation=dilation, padding=(kernel_list[0]-1)*dilation)\n",
    "        self.t_conv_1 = nn.Conv1d(itcn_d, itcn_d//3, kernel_list[1], dilation=dilation, padding=(kernel_list[1]-1)*dilation)\n",
    "        self.t_conv_2 = nn.Conv1d(itcn_d, itcn_d//3, kernel_list[2], dilation=dilation, padding=(kernel_list[2]-1)*dilation)\n",
    "        self.bn = nn.BatchNorm1d(itcn_d)\n",
    "\n",
    "    def forward(self, x_split, dilation):\n",
    "        x_cat = [\n",
    "            self.clip_end(self.t_conv_0(x_split), 0, dilation),\n",
    "            self.clip_end(self.t_conv_1(x_split), 1, dilation),\n",
    "            self.clip_end(self.t_conv_2(x_split), 2, dilation)\n",
    "        ]\n",
    "        x_cat = torch.cat(x_cat, 1)\n",
    "        x_out = F.relu(self.bn(x_cat))\n",
    "        return x_out\n",
    "\n",
    "    def clip_end(self, x, i, dilation):\n",
    "        padding = (kernel_list[i] - 1) * dilation\n",
    "        x = x[:, :, :-padding].contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "class ITCN(nn.Module):\n",
    "    def __init__(self, batch_size, n_neurons, itcn_d, t_repetition, kernel_list):\n",
    "        super(ITCN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_neurons = n_neurons\n",
    "        self.itcn_d = itcn_d\n",
    "        self.t_repetition = t_repetition\n",
    "        self.kernel_list = kernel_list\n",
    "        self.inception_tc_2 = InceptionTC(itcn_d, 2)  \n",
    "        self.inception_tc_4 = InceptionTC(itcn_d, 4)\n",
    "        self.inception_tc_6 = InceptionTC(itcn_d, 6)\n",
    "\n",
    "    def forward(self, x_split):\n",
    "        x_split = x_split.reshape(self.batch_size * self.n_neurons, self.itcn_d, self.t_repetition)\n",
    "        x_split = self.inception_tc_2(x_split, 2)\n",
    "        x_split = self.inception_tc_4(x_split, 4)\n",
    "        x_split = self.inception_tc_6(x_split, 6)\n",
    "        x_split = x_split.reshape(self.batch_size, self.t_repetition, self.n_neurons, self.itcn_d)\n",
    "        return x_split\n",
    "\n",
    "\n",
    "class RegionEmbedding(nn.Module):\n",
    "    def __init__(self, len_window, itcn_d, ebd_d, batch_size, n_neurons, t_repetition, kernel_list):\n",
    "        super(RegionEmbedding, self).__init__()\n",
    "        self.input_layer = nn.Linear(len_window, itcn_d)\n",
    "        self.itcn_layer = ITCN(batch_size, n_neurons, itcn_d, t_repetition, kernel_list)\n",
    "        self.output_fc = nn.Sequential(nn.Linear(itcn_d, itcn_d), nn.ReLU(), nn.Linear(itcn_d, ebd_d))\n",
    "\n",
    "    def forward(self, x_split):\n",
    "        x_split = self.input_layer(x_split)\n",
    "        x_split = self.itcn_layer(x_split)\n",
    "        x_split = self.output_fc(x_split)\n",
    "        return x_split\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, n_neurons, tau):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        n_neurons_ebd = int(tau * n_neurons)\n",
    "        self.spatial_attn = nn.Sequential(\n",
    "            nn.Linear(n_neurons, n_neurons_ebd, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons_ebd, n_neurons, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_ebd):\n",
    "        x_spatial_attn = torch.mean(x_ebd, -1)\n",
    "        x_spatial_attn = self.spatial_attn(x_spatial_attn)\n",
    "        x_spatial_attn = x_spatial_attn.unsqueeze(-1)\n",
    "        return x_spatial_attn\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, t_repetition, tau, n_neurons, ebd_d):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        T_ebd = int(tau * t_repetition)\n",
    "        self.temporal_attn = nn.Sequential(\n",
    "            nn.Linear(t_repetition, T_ebd, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(T_ebd, t_repetition, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_ebd):\n",
    "        x_temporal_attn = x_ebd.view(-1, t_repetition, n_neurons * ebd_d)\n",
    "        x_temporal_attn = torch.mean(x_temporal_attn, -1)\n",
    "        x_temporal_attn = self.temporal_attn(x_temporal_attn)\n",
    "        x_temporal_attn = x_temporal_attn.view(-1, t_repetition, 1, 1)\n",
    "        return x_temporal_attn\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, len_window, itcn_d, ebd_d, batch_size, n_neurons, t_repetition, tau, kernel_list):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.region_embd = RegionEmbedding(len_window, itcn_d, ebd_d, batch_size, n_neurons, t_repetition, kernel_list)\n",
    "        self.spat_attention = SpatialAttention(n_neurons, tau)\n",
    "        self.temp_attention = TemporalAttention(t_repetition, tau, n_neurons, ebd_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ebd = self.region_embd(x)\n",
    "        x_spatial_attention = self.spat_attention(x_ebd)\n",
    "        x_ebd = x_spatial_attention * x_ebd\n",
    "        x_temporal_attn = self.temp_attention(x_ebd)\n",
    "        x_ebd = x_temporal_attn * x_ebd\n",
    "        return x_ebd\n",
    "\n",
    "\n",
    "# Instantiate your model\n",
    "model = MyModel(len_window, itcn_d, ebd_d, batch_size, n_neurons, t_repetition, tau, kernel_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 0.10520316660404205\n",
      "Epoch 2/100, Average Loss: 0.10508951172232628\n",
      "Epoch 3/100, Average Loss: 0.10506994277238846\n",
      "Epoch 4/100, Average Loss: 0.10506955813616514\n",
      "Epoch 5/100, Average Loss: 0.1050680112093687\n",
      "Epoch 6/100, Average Loss: 0.10506728012114763\n",
      "Epoch 7/100, Average Loss: 0.10506681445986032\n",
      "Epoch 8/100, Average Loss: 0.10506607312709093\n",
      "Epoch 9/100, Average Loss: 0.10506535414606333\n",
      "Epoch 10/100, Average Loss: 0.10506449267268181\n",
      "Epoch 11/100, Average Loss: 0.10506337322294712\n",
      "Epoch 12/100, Average Loss: 0.10506176017224789\n",
      "Epoch 13/100, Average Loss: 0.10505902674049139\n",
      "Epoch 14/100, Average Loss: 0.10505409725010395\n",
      "Epoch 15/100, Average Loss: 0.1050462992861867\n",
      "Epoch 16/100, Average Loss: 0.10503544006496668\n",
      "Epoch 17/100, Average Loss: 0.10502195823937654\n",
      "Epoch 18/100, Average Loss: 0.10500869806855917\n",
      "Epoch 19/100, Average Loss: 0.10499266162514687\n",
      "Epoch 20/100, Average Loss: 0.10497252736240625\n",
      "Epoch 21/100, Average Loss: 0.10494766756892204\n",
      "Epoch 22/100, Average Loss: 0.10491569433361292\n",
      "Epoch 23/100, Average Loss: 0.10487017594277859\n",
      "Epoch 24/100, Average Loss: 0.10479899030178785\n",
      "Epoch 25/100, Average Loss: 0.10468897223472595\n",
      "Epoch 26/100, Average Loss: 0.10457943379878998\n",
      "Epoch 27/100, Average Loss: 0.10450390260666609\n",
      "Epoch 28/100, Average Loss: 0.10445464123040438\n",
      "Epoch 29/100, Average Loss: 0.10442299488931894\n",
      "Epoch 30/100, Average Loss: 0.10439124051481485\n",
      "Epoch 31/100, Average Loss: 0.10437029413878918\n",
      "Epoch 32/100, Average Loss: 0.10435492265969515\n",
      "Epoch 33/100, Average Loss: 0.10434499941766262\n",
      "Epoch 34/100, Average Loss: 0.10433508735150099\n",
      "Epoch 35/100, Average Loss: 0.10432553850114346\n",
      "Epoch 36/100, Average Loss: 0.10431813262403011\n",
      "Epoch 37/100, Average Loss: 0.10431295540183783\n",
      "Epoch 38/100, Average Loss: 0.104308539070189\n",
      "Epoch 39/100, Average Loss: 0.10430545639246702\n",
      "Epoch 40/100, Average Loss: 0.10430199932307005\n",
      "Epoch 41/100, Average Loss: 0.10429923422634602\n",
      "Epoch 42/100, Average Loss: 0.10429709497839212\n",
      "Epoch 43/100, Average Loss: 0.10429555550217628\n",
      "Epoch 44/100, Average Loss: 0.10429386328905821\n",
      "Epoch 45/100, Average Loss: 0.1042913543060422\n",
      "Epoch 46/100, Average Loss: 0.10428927652537823\n",
      "Epoch 47/100, Average Loss: 0.10428801830857992\n",
      "Epoch 48/100, Average Loss: 0.10428719222545624\n",
      "Epoch 49/100, Average Loss: 0.1042866725474596\n",
      "Epoch 50/100, Average Loss: 0.10428649093955755\n",
      "Epoch 51/100, Average Loss: 0.10428565554320812\n",
      "Epoch 52/100, Average Loss: 0.10428354889154434\n",
      "Epoch 53/100, Average Loss: 0.10428084060549736\n",
      "Epoch 54/100, Average Loss: 0.10427870601415634\n",
      "Epoch 55/100, Average Loss: 0.10427735466510057\n",
      "Epoch 56/100, Average Loss: 0.10427606198936701\n",
      "Epoch 57/100, Average Loss: 0.10427518654614687\n",
      "Epoch 58/100, Average Loss: 0.10427431762218475\n",
      "Epoch 59/100, Average Loss: 0.10427325032651424\n",
      "Epoch 60/100, Average Loss: 0.10427108127623796\n",
      "Epoch 61/100, Average Loss: 0.10427019651979208\n",
      "Epoch 62/100, Average Loss: 0.10426993481814861\n",
      "Epoch 63/100, Average Loss: 0.10426981654018164\n",
      "Epoch 64/100, Average Loss: 0.10426921676844358\n",
      "Epoch 65/100, Average Loss: 0.10426804330199957\n",
      "Epoch 66/100, Average Loss: 0.10426667984575033\n",
      "Epoch 67/100, Average Loss: 0.1042651766911149\n",
      "Epoch 68/100, Average Loss: 0.10426376108080149\n",
      "Epoch 69/100, Average Loss: 0.10426245722919703\n",
      "Epoch 70/100, Average Loss: 0.10426161251962185\n",
      "Epoch 71/100, Average Loss: 0.10426072776317596\n",
      "Epoch 72/100, Average Loss: 0.10425993986427784\n",
      "Epoch 73/100, Average Loss: 0.10425891727209091\n",
      "Epoch 74/100, Average Loss: 0.10425660386681557\n",
      "Epoch 75/100, Average Loss: 0.104255479760468\n",
      "Epoch 76/100, Average Loss: 0.10425431281328201\n",
      "Epoch 77/100, Average Loss: 0.10425250511616468\n",
      "Epoch 78/100, Average Loss: 0.10425139404833317\n",
      "Epoch 79/100, Average Loss: 0.1042509563267231\n",
      "Epoch 80/100, Average Loss: 0.10425079707056284\n",
      "Epoch 81/100, Average Loss: 0.10425069276243448\n",
      "Epoch 82/100, Average Loss: 0.10425009485334158\n",
      "Epoch 83/100, Average Loss: 0.1042489567771554\n",
      "Epoch 84/100, Average Loss: 0.10424822568893433\n",
      "Epoch 85/100, Average Loss: 0.10424693860113621\n",
      "Epoch 86/100, Average Loss: 0.10424612369388342\n",
      "Epoch 87/100, Average Loss: 0.10424575675278902\n",
      "Epoch 88/100, Average Loss: 0.10424540471285582\n",
      "Epoch 89/100, Average Loss: 0.10424495860934258\n",
      "Epoch 90/100, Average Loss: 0.10424432624131441\n",
      "Epoch 91/100, Average Loss: 0.10424348153173923\n",
      "Epoch 92/100, Average Loss: 0.10424273926764727\n",
      "Epoch 93/100, Average Loss: 0.10424222983419895\n",
      "Epoch 94/100, Average Loss: 0.10424165707081556\n",
      "Epoch 95/100, Average Loss: 0.10424091015011072\n",
      "Epoch 96/100, Average Loss: 0.10424026194959879\n",
      "Epoch 97/100, Average Loss: 0.10423970222473145\n",
      "Epoch 98/100, Average Loss: 0.1042391350492835\n",
      "Epoch 99/100, Average Loss: 0.104238610714674\n",
      "Epoch 100/100, Average Loss: 0.10423823725432158\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_input = input_tensor[0:240]\n",
    "test_input = input_tensor[241:300]\n",
    "train_ground = ground_truth_tens[0:240]\n",
    "test_ground = ground_truth_tens[241:300]\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "n_samples_train = train_input.size(0)\n",
    "n_batches_train = n_samples_train // batch_size\n",
    "\n",
    "for epoch in range(n_episodes):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for i in range(n_batches_train):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        x_split_batch = train_input[start_idx:end_idx]\n",
    "        ground_truth_batch = train_ground[start_idx:end_idx]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_split_batch)\n",
    "        adjacency_matrix = construct_graph(outputs)\n",
    "        sparse_adjacency = sparsify(adjacency_matrix)\n",
    "        loss = criterion(sparse_adjacency, ground_truth_batch)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / n_batches_train\n",
    "    print(f'Epoch {epoch + 1}/{n_episodes}, Average Loss: {average_loss}')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing - Average Loss: 0.09685724973678589\n"
     ]
    }
   ],
   "source": [
    "n_samples_test = test_input.size(0)\n",
    "n_batches_test = n_samples_test // batch_size\n",
    "\n",
    "total_loss_test = 0.0\n",
    "\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation during testing\n",
    "    for i in range(n_batches_test):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        x_split_batch_test = test_input[start_idx:end_idx]\n",
    "        ground_truth_batch_test = test_ground[start_idx:end_idx]\n",
    "\n",
    "        outputs_test = model(x_split_batch_test)\n",
    "        adjacency_matrix_test = construct_graph(outputs_test)\n",
    "        sparse_adjacency_test = sparsify(adjacency_matrix_test)\n",
    "        loss_test = criterion(sparse_adjacency_test, ground_truth_batch_test)\n",
    "        total_loss_test += loss_test.item()\n",
    "\n",
    "average_loss_test = total_loss_test / n_batches_test\n",
    "print(f'Testing - Average Loss: {average_loss_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One subject only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_split shape:  torch.Size([1, 39, 360, 30])\n",
      "fc_matrices_dgbsl SHAPE:  torch.Size([1, 39, 360, 360])\n",
      "Epoch 1/1000, Loss: 0.09225441515445709\n",
      "Epoch 2/1000, Loss: 0.09214719384908676\n",
      "Epoch 3/1000, Loss: 0.09208934009075165\n",
      "Epoch 4/1000, Loss: 0.09204959869384766\n",
      "Epoch 5/1000, Loss: 0.09202088415622711\n",
      "Epoch 6/1000, Loss: 0.09199746698141098\n",
      "Epoch 7/1000, Loss: 0.09197724610567093\n",
      "Epoch 8/1000, Loss: 0.09195952117443085\n",
      "Epoch 9/1000, Loss: 0.09194444119930267\n",
      "Epoch 10/1000, Loss: 0.09193232655525208\n",
      "Epoch 11/1000, Loss: 0.09192299097776413\n",
      "Epoch 12/1000, Loss: 0.09191613644361496\n",
      "Epoch 13/1000, Loss: 0.09191137552261353\n",
      "Epoch 14/1000, Loss: 0.09190823137760162\n",
      "Epoch 15/1000, Loss: 0.09190625697374344\n",
      "Epoch 16/1000, Loss: 0.09190506488084793\n",
      "Epoch 17/1000, Loss: 0.09190438687801361\n",
      "Epoch 18/1000, Loss: 0.09190399944782257\n",
      "Epoch 19/1000, Loss: 0.09190378338098526\n",
      "Epoch 20/1000, Loss: 0.0919036790728569\n",
      "Epoch 21/1000, Loss: 0.09190360456705093\n",
      "Epoch 22/1000, Loss: 0.09190356731414795\n",
      "Epoch 23/1000, Loss: 0.09190353006124496\n",
      "Epoch 24/1000, Loss: 0.09190350025892258\n",
      "Epoch 25/1000, Loss: 0.091903455555439\n",
      "Epoch 26/1000, Loss: 0.09190341085195541\n",
      "Epoch 27/1000, Loss: 0.09190335124731064\n",
      "Epoch 28/1000, Loss: 0.09190328419208527\n",
      "Epoch 29/1000, Loss: 0.0919032022356987\n",
      "Epoch 30/1000, Loss: 0.09190311282873154\n",
      "Epoch 31/1000, Loss: 0.09190301597118378\n",
      "Epoch 32/1000, Loss: 0.09190288931131363\n",
      "Epoch 33/1000, Loss: 0.09190274775028229\n",
      "Epoch 34/1000, Loss: 0.09190258383750916\n",
      "Epoch 35/1000, Loss: 0.09190240502357483\n",
      "Epoch 36/1000, Loss: 0.09190217405557632\n",
      "Epoch 37/1000, Loss: 0.09190190583467484\n",
      "Epoch 38/1000, Loss: 0.09190157800912857\n",
      "Epoch 39/1000, Loss: 0.09190120548009872\n",
      "Epoch 40/1000, Loss: 0.09190074354410172\n",
      "Epoch 41/1000, Loss: 0.09190018475055695\n",
      "Epoch 42/1000, Loss: 0.09189948439598083\n",
      "Epoch 43/1000, Loss: 0.09189865738153458\n",
      "Epoch 44/1000, Loss: 0.0918976292014122\n",
      "Epoch 45/1000, Loss: 0.09189637750387192\n",
      "Epoch 46/1000, Loss: 0.09189486503601074\n",
      "Epoch 47/1000, Loss: 0.0918930396437645\n",
      "Epoch 48/1000, Loss: 0.0918908417224884\n",
      "Epoch 49/1000, Loss: 0.0918881967663765\n",
      "Epoch 50/1000, Loss: 0.09188500046730042\n",
      "Epoch 51/1000, Loss: 0.091881163418293\n",
      "Epoch 52/1000, Loss: 0.0918765515089035\n",
      "Epoch 53/1000, Loss: 0.0918710008263588\n",
      "Epoch 54/1000, Loss: 0.09186425060033798\n",
      "Epoch 55/1000, Loss: 0.09185604751110077\n",
      "Epoch 56/1000, Loss: 0.09184618294239044\n",
      "Epoch 57/1000, Loss: 0.09183424711227417\n",
      "Epoch 58/1000, Loss: 0.09181978553533554\n",
      "Epoch 59/1000, Loss: 0.09180251508951187\n",
      "Epoch 60/1000, Loss: 0.09178240597248077\n",
      "Epoch 61/1000, Loss: 0.09175991266965866\n",
      "Epoch 62/1000, Loss: 0.09173519909381866\n",
      "Epoch 63/1000, Loss: 0.09170820564031601\n",
      "Epoch 64/1000, Loss: 0.09167876094579697\n",
      "Epoch 65/1000, Loss: 0.09164655953645706\n",
      "Epoch 66/1000, Loss: 0.09161131083965302\n",
      "Epoch 67/1000, Loss: 0.09157313406467438\n",
      "Epoch 68/1000, Loss: 0.09153185784816742\n",
      "Epoch 69/1000, Loss: 0.09148702770471573\n",
      "Epoch 70/1000, Loss: 0.09143822640180588\n",
      "Epoch 71/1000, Loss: 0.09138534963130951\n",
      "Epoch 72/1000, Loss: 0.09132836014032364\n",
      "Epoch 73/1000, Loss: 0.09126660972833633\n",
      "Epoch 74/1000, Loss: 0.0912015438079834\n",
      "Epoch 75/1000, Loss: 0.0911366194486618\n",
      "Epoch 76/1000, Loss: 0.09107398986816406\n",
      "Epoch 77/1000, Loss: 0.09101460874080658\n",
      "Epoch 78/1000, Loss: 0.09095606952905655\n",
      "Epoch 79/1000, Loss: 0.0908961221575737\n",
      "Epoch 80/1000, Loss: 0.0908363088965416\n",
      "Epoch 81/1000, Loss: 0.09077782183885574\n",
      "Epoch 82/1000, Loss: 0.09071902185678482\n",
      "Epoch 83/1000, Loss: 0.09066173434257507\n",
      "Epoch 84/1000, Loss: 0.09060755372047424\n",
      "Epoch 85/1000, Loss: 0.0905599370598793\n",
      "Epoch 86/1000, Loss: 0.09051774442195892\n",
      "Epoch 87/1000, Loss: 0.09047742187976837\n",
      "Epoch 88/1000, Loss: 0.09043888002634048\n",
      "Epoch 89/1000, Loss: 0.09040466696023941\n",
      "Epoch 90/1000, Loss: 0.0903700515627861\n",
      "Epoch 91/1000, Loss: 0.09032608568668365\n",
      "Epoch 92/1000, Loss: 0.09027685225009918\n",
      "Epoch 93/1000, Loss: 0.09023265540599823\n",
      "Epoch 94/1000, Loss: 0.09019812941551208\n",
      "Epoch 95/1000, Loss: 0.09016107022762299\n",
      "Epoch 96/1000, Loss: 0.09012079983949661\n",
      "Epoch 97/1000, Loss: 0.0900813639163971\n",
      "Epoch 98/1000, Loss: 0.0900462418794632\n",
      "Epoch 99/1000, Loss: 0.0900091752409935\n",
      "Epoch 100/1000, Loss: 0.08997005969285965\n",
      "Epoch 101/1000, Loss: 0.0899292603135109\n",
      "Epoch 102/1000, Loss: 0.08988738805055618\n",
      "Epoch 103/1000, Loss: 0.08984296768903732\n",
      "Epoch 104/1000, Loss: 0.08980923891067505\n",
      "Epoch 105/1000, Loss: 0.0897778645157814\n",
      "Epoch 106/1000, Loss: 0.0897456556558609\n",
      "Epoch 107/1000, Loss: 0.08971302956342697\n",
      "Epoch 108/1000, Loss: 0.08968045562505722\n",
      "Epoch 109/1000, Loss: 0.08964835852384567\n",
      "Epoch 110/1000, Loss: 0.08961918950080872\n",
      "Epoch 111/1000, Loss: 0.08958518505096436\n",
      "Epoch 112/1000, Loss: 0.0895543321967125\n",
      "Epoch 113/1000, Loss: 0.0895238071680069\n",
      "Epoch 114/1000, Loss: 0.08948969095945358\n",
      "Epoch 115/1000, Loss: 0.08945389091968536\n",
      "Epoch 116/1000, Loss: 0.08942144364118576\n",
      "Epoch 117/1000, Loss: 0.08939054608345032\n",
      "Epoch 118/1000, Loss: 0.08935663849115372\n",
      "Epoch 119/1000, Loss: 0.08932366222143173\n",
      "Epoch 120/1000, Loss: 0.0892927348613739\n",
      "Epoch 121/1000, Loss: 0.08926276862621307\n",
      "Epoch 122/1000, Loss: 0.08923009783029556\n",
      "Epoch 123/1000, Loss: 0.08919613063335419\n",
      "Epoch 124/1000, Loss: 0.08916521817445755\n",
      "Epoch 125/1000, Loss: 0.08913815021514893\n",
      "Epoch 126/1000, Loss: 0.08911111205816269\n",
      "Epoch 127/1000, Loss: 0.08907732367515564\n",
      "Epoch 128/1000, Loss: 0.08904397487640381\n",
      "Epoch 129/1000, Loss: 0.08901955187320709\n",
      "Epoch 130/1000, Loss: 0.08899345993995667\n",
      "Epoch 131/1000, Loss: 0.08895952999591827\n",
      "Epoch 132/1000, Loss: 0.0889289528131485\n",
      "Epoch 133/1000, Loss: 0.0889037698507309\n",
      "Epoch 134/1000, Loss: 0.08887643367052078\n",
      "Epoch 135/1000, Loss: 0.08884533494710922\n",
      "Epoch 136/1000, Loss: 0.08881515264511108\n",
      "Epoch 137/1000, Loss: 0.08878890424966812\n",
      "Epoch 138/1000, Loss: 0.08876283466815948\n",
      "Epoch 139/1000, Loss: 0.08873723447322845\n",
      "Epoch 140/1000, Loss: 0.08870652318000793\n",
      "Epoch 141/1000, Loss: 0.0886778011918068\n",
      "Epoch 142/1000, Loss: 0.08864860236644745\n",
      "Epoch 143/1000, Loss: 0.08862227946519852\n",
      "Epoch 144/1000, Loss: 0.08859588205814362\n",
      "Epoch 145/1000, Loss: 0.08857109397649765\n",
      "Epoch 146/1000, Loss: 0.08855052292346954\n",
      "Epoch 147/1000, Loss: 0.08853822201490402\n",
      "Epoch 148/1000, Loss: 0.08853276818990707\n",
      "Epoch 149/1000, Loss: 0.08848738670349121\n",
      "Epoch 150/1000, Loss: 0.08845637738704681\n",
      "Epoch 151/1000, Loss: 0.08845144510269165\n",
      "Epoch 152/1000, Loss: 0.08841812610626221\n",
      "Epoch 153/1000, Loss: 0.08839584141969681\n",
      "Epoch 154/1000, Loss: 0.08838232606649399\n",
      "Epoch 155/1000, Loss: 0.08835498243570328\n",
      "Epoch 156/1000, Loss: 0.08833636343479156\n",
      "Epoch 157/1000, Loss: 0.08832401782274246\n",
      "Epoch 158/1000, Loss: 0.08830016851425171\n",
      "Epoch 159/1000, Loss: 0.08827941119670868\n",
      "Epoch 160/1000, Loss: 0.08826557546854019\n",
      "Epoch 161/1000, Loss: 0.08825092762708664\n",
      "Epoch 162/1000, Loss: 0.08823496103286743\n",
      "Epoch 163/1000, Loss: 0.0882154256105423\n",
      "Epoch 164/1000, Loss: 0.0882011130452156\n",
      "Epoch 165/1000, Loss: 0.08818969130516052\n",
      "Epoch 166/1000, Loss: 0.08817925304174423\n",
      "Epoch 167/1000, Loss: 0.08816303312778473\n",
      "Epoch 168/1000, Loss: 0.08814629912376404\n",
      "Epoch 169/1000, Loss: 0.0881318598985672\n",
      "Epoch 170/1000, Loss: 0.08811947703361511\n",
      "Epoch 171/1000, Loss: 0.088109090924263\n",
      "Epoch 172/1000, Loss: 0.0881020650267601\n",
      "Epoch 173/1000, Loss: 0.08809199184179306\n",
      "Epoch 174/1000, Loss: 0.0880822017788887\n",
      "Epoch 175/1000, Loss: 0.08806687593460083\n",
      "Epoch 176/1000, Loss: 0.08805385231971741\n",
      "Epoch 177/1000, Loss: 0.08804374933242798\n",
      "Epoch 178/1000, Loss: 0.08803382515907288\n",
      "Epoch 179/1000, Loss: 0.08802542090415955\n",
      "Epoch 180/1000, Loss: 0.08801886439323425\n",
      "Epoch 181/1000, Loss: 0.08801475167274475\n",
      "Epoch 182/1000, Loss: 0.08802152425050735\n",
      "Epoch 183/1000, Loss: 0.08805036544799805\n",
      "Epoch 184/1000, Loss: 0.08803313970565796\n",
      "Epoch 185/1000, Loss: 0.08799321949481964\n",
      "Epoch 186/1000, Loss: 0.08798549324274063\n",
      "Epoch 187/1000, Loss: 0.08799917250871658\n",
      "Epoch 188/1000, Loss: 0.08797497302293777\n",
      "Epoch 189/1000, Loss: 0.08796674013137817\n",
      "Epoch 190/1000, Loss: 0.08797596395015717\n",
      "Epoch 191/1000, Loss: 0.08796125650405884\n",
      "Epoch 192/1000, Loss: 0.08795426040887833\n",
      "Epoch 193/1000, Loss: 0.08795386552810669\n",
      "Epoch 194/1000, Loss: 0.08794207125902176\n",
      "Epoch 195/1000, Loss: 0.08793541043996811\n",
      "Epoch 196/1000, Loss: 0.08793531358242035\n",
      "Epoch 197/1000, Loss: 0.08792011439800262\n",
      "Epoch 198/1000, Loss: 0.08791952580213547\n",
      "Epoch 199/1000, Loss: 0.08792014420032501\n",
      "Epoch 200/1000, Loss: 0.08790649473667145\n",
      "Epoch 201/1000, Loss: 0.08790421485900879\n",
      "Epoch 202/1000, Loss: 0.08790481090545654\n",
      "Epoch 203/1000, Loss: 0.0878944844007492\n",
      "Epoch 204/1000, Loss: 0.08789375424385071\n",
      "Epoch 205/1000, Loss: 0.08788669109344482\n",
      "Epoch 206/1000, Loss: 0.08787965774536133\n",
      "Epoch 207/1000, Loss: 0.08787763118743896\n",
      "Epoch 208/1000, Loss: 0.08787301927804947\n",
      "Epoch 209/1000, Loss: 0.08786913752555847\n",
      "Epoch 210/1000, Loss: 0.08787238597869873\n",
      "Epoch 211/1000, Loss: 0.08787386864423752\n",
      "Epoch 212/1000, Loss: 0.08787240087985992\n",
      "Epoch 213/1000, Loss: 0.08787130564451218\n",
      "Epoch 214/1000, Loss: 0.08785504847764969\n",
      "Epoch 215/1000, Loss: 0.08785077929496765\n",
      "Epoch 216/1000, Loss: 0.08785124123096466\n",
      "Epoch 217/1000, Loss: 0.08784623444080353\n",
      "Epoch 218/1000, Loss: 0.08784797787666321\n",
      "Epoch 219/1000, Loss: 0.08784177899360657\n",
      "Epoch 220/1000, Loss: 0.08782906830310822\n",
      "Epoch 221/1000, Loss: 0.08782657235860825\n",
      "Epoch 222/1000, Loss: 0.08782913535833359\n",
      "Epoch 223/1000, Loss: 0.08783126622438431\n",
      "Epoch 224/1000, Loss: 0.08782276511192322\n",
      "Epoch 225/1000, Loss: 0.08781298995018005\n",
      "Epoch 226/1000, Loss: 0.08780845254659653\n",
      "Epoch 227/1000, Loss: 0.08780164271593094\n",
      "Epoch 228/1000, Loss: 0.08780406415462494\n",
      "Epoch 229/1000, Loss: 0.08780472725629807\n",
      "Epoch 230/1000, Loss: 0.08780091255903244\n",
      "Epoch 231/1000, Loss: 0.08780308067798615\n",
      "Epoch 232/1000, Loss: 0.08778995275497437\n",
      "Epoch 233/1000, Loss: 0.0877813845872879\n",
      "Epoch 234/1000, Loss: 0.08777804672718048\n",
      "Epoch 235/1000, Loss: 0.08778081834316254\n",
      "Epoch 236/1000, Loss: 0.08778265863656998\n",
      "Epoch 237/1000, Loss: 0.08778306841850281\n",
      "Epoch 238/1000, Loss: 0.08777347207069397\n",
      "Epoch 239/1000, Loss: 0.08776602149009705\n",
      "Epoch 240/1000, Loss: 0.0877622738480568\n",
      "Epoch 241/1000, Loss: 0.0877581387758255\n",
      "Epoch 242/1000, Loss: 0.08775275200605392\n",
      "Epoch 243/1000, Loss: 0.08775851130485535\n",
      "Epoch 244/1000, Loss: 0.08776387572288513\n",
      "Epoch 245/1000, Loss: 0.08776877075433731\n",
      "Epoch 246/1000, Loss: 0.08776313811540604\n",
      "Epoch 247/1000, Loss: 0.0877428725361824\n",
      "Epoch 248/1000, Loss: 0.08773590624332428\n",
      "Epoch 249/1000, Loss: 0.08774504065513611\n",
      "Epoch 250/1000, Loss: 0.0877414420247078\n",
      "Epoch 251/1000, Loss: 0.08773571997880936\n",
      "Epoch 252/1000, Loss: 0.0877421498298645\n",
      "Epoch 253/1000, Loss: 0.08773916214704514\n",
      "Epoch 254/1000, Loss: 0.0877295434474945\n",
      "Epoch 255/1000, Loss: 0.0877179503440857\n",
      "Epoch 256/1000, Loss: 0.08771675080060959\n",
      "Epoch 257/1000, Loss: 0.0877193734049797\n",
      "Epoch 258/1000, Loss: 0.08771692216396332\n",
      "Epoch 259/1000, Loss: 0.08771238476037979\n",
      "Epoch 260/1000, Loss: 0.08771403133869171\n",
      "Epoch 261/1000, Loss: 0.08771614730358124\n",
      "Epoch 262/1000, Loss: 0.08771099895238876\n",
      "Epoch 263/1000, Loss: 0.08770349621772766\n",
      "Epoch 264/1000, Loss: 0.08769726008176804\n",
      "Epoch 265/1000, Loss: 0.08769337832927704\n",
      "Epoch 266/1000, Loss: 0.08769139647483826\n",
      "Epoch 267/1000, Loss: 0.08768579363822937\n",
      "Epoch 268/1000, Loss: 0.08768299967050552\n",
      "Epoch 269/1000, Loss: 0.08768405020236969\n",
      "Epoch 270/1000, Loss: 0.08768749982118607\n",
      "Epoch 271/1000, Loss: 0.0877007320523262\n",
      "Epoch 272/1000, Loss: 0.08771787583827972\n",
      "Epoch 273/1000, Loss: 0.08773408085107803\n",
      "Epoch 274/1000, Loss: 0.08771362155675888\n",
      "Epoch 275/1000, Loss: 0.08768226206302643\n",
      "Epoch 276/1000, Loss: 0.08767484873533249\n",
      "Epoch 277/1000, Loss: 0.08769052475690842\n",
      "Epoch 278/1000, Loss: 0.08770023286342621\n",
      "Epoch 279/1000, Loss: 0.08767855167388916\n",
      "Epoch 280/1000, Loss: 0.08766727149486542\n",
      "Epoch 281/1000, Loss: 0.08767111599445343\n",
      "Epoch 282/1000, Loss: 0.08767499029636383\n",
      "Epoch 283/1000, Loss: 0.08766644448041916\n",
      "Epoch 284/1000, Loss: 0.08765313029289246\n",
      "Epoch 285/1000, Loss: 0.08765731006860733\n",
      "Epoch 286/1000, Loss: 0.08766577392816544\n",
      "Epoch 287/1000, Loss: 0.08765357732772827\n",
      "Epoch 288/1000, Loss: 0.08764041215181351\n",
      "Epoch 289/1000, Loss: 0.08764689415693283\n",
      "Epoch 290/1000, Loss: 0.0876481682062149\n",
      "Epoch 291/1000, Loss: 0.08764471858739853\n",
      "Epoch 292/1000, Loss: 0.0876430943608284\n",
      "Epoch 293/1000, Loss: 0.08763778209686279\n",
      "Epoch 294/1000, Loss: 0.08762974292039871\n",
      "Epoch 295/1000, Loss: 0.08763402700424194\n",
      "Epoch 296/1000, Loss: 0.08764158189296722\n",
      "Epoch 297/1000, Loss: 0.08763112872838974\n",
      "Epoch 298/1000, Loss: 0.08762559294700623\n",
      "Epoch 299/1000, Loss: 0.08762653172016144\n",
      "Epoch 300/1000, Loss: 0.0876196101307869\n",
      "Epoch 301/1000, Loss: 0.087620809674263\n",
      "Epoch 302/1000, Loss: 0.08762570470571518\n",
      "Epoch 303/1000, Loss: 0.08761975169181824\n",
      "Epoch 304/1000, Loss: 0.08761902153491974\n",
      "Epoch 305/1000, Loss: 0.08762051910161972\n",
      "Epoch 306/1000, Loss: 0.08760987222194672\n",
      "Epoch 307/1000, Loss: 0.08760993182659149\n",
      "Epoch 308/1000, Loss: 0.0876101478934288\n",
      "Epoch 309/1000, Loss: 0.08760224282741547\n",
      "Epoch 310/1000, Loss: 0.08759882301092148\n",
      "Epoch 311/1000, Loss: 0.08759957551956177\n",
      "Epoch 312/1000, Loss: 0.08760017156600952\n",
      "Epoch 313/1000, Loss: 0.08760418742895126\n",
      "Epoch 314/1000, Loss: 0.08761397749185562\n",
      "Epoch 315/1000, Loss: 0.0876278430223465\n",
      "Epoch 316/1000, Loss: 0.08763563632965088\n",
      "Epoch 317/1000, Loss: 0.0876397043466568\n",
      "Epoch 318/1000, Loss: 0.08763465285301208\n",
      "Epoch 319/1000, Loss: 0.08763949573040009\n",
      "Epoch 320/1000, Loss: 0.08760542422533035\n",
      "Epoch 321/1000, Loss: 0.08761201798915863\n",
      "Epoch 322/1000, Loss: 0.08764157444238663\n",
      "Epoch 323/1000, Loss: 0.0876041129231453\n",
      "Epoch 324/1000, Loss: 0.08760550618171692\n",
      "Epoch 325/1000, Loss: 0.08761698752641678\n",
      "Epoch 326/1000, Loss: 0.08759250491857529\n",
      "Epoch 327/1000, Loss: 0.08759892731904984\n",
      "Epoch 328/1000, Loss: 0.08760149031877518\n",
      "Epoch 329/1000, Loss: 0.08758389949798584\n",
      "Epoch 330/1000, Loss: 0.08759412914514542\n",
      "Epoch 331/1000, Loss: 0.08758911490440369\n",
      "Epoch 332/1000, Loss: 0.08757944405078888\n",
      "Epoch 333/1000, Loss: 0.08759325742721558\n",
      "Epoch 334/1000, Loss: 0.08757828921079636\n",
      "Epoch 335/1000, Loss: 0.08757471293210983\n",
      "Epoch 336/1000, Loss: 0.08758606761693954\n",
      "Epoch 337/1000, Loss: 0.08756553381681442\n",
      "Epoch 338/1000, Loss: 0.08758224546909332\n",
      "Epoch 339/1000, Loss: 0.08758377283811569\n",
      "Epoch 340/1000, Loss: 0.08757676184177399\n",
      "Epoch 341/1000, Loss: 0.08758142590522766\n",
      "Epoch 342/1000, Loss: 0.0875636413693428\n",
      "Epoch 343/1000, Loss: 0.0875648483633995\n",
      "Epoch 344/1000, Loss: 0.08756699413061142\n",
      "Epoch 345/1000, Loss: 0.08756314963102341\n",
      "Epoch 346/1000, Loss: 0.08757245540618896\n",
      "Epoch 347/1000, Loss: 0.08756102621555328\n",
      "Epoch 348/1000, Loss: 0.0875672847032547\n",
      "Epoch 349/1000, Loss: 0.08755508065223694\n",
      "Epoch 350/1000, Loss: 0.08755221217870712\n",
      "Epoch 351/1000, Loss: 0.08755308389663696\n",
      "Epoch 352/1000, Loss: 0.08754252642393112\n",
      "Epoch 353/1000, Loss: 0.08754885941743851\n",
      "Epoch 354/1000, Loss: 0.08754773437976837\n",
      "Epoch 355/1000, Loss: 0.08754399418830872\n",
      "Epoch 356/1000, Loss: 0.08754787594079971\n",
      "Epoch 357/1000, Loss: 0.08754590153694153\n",
      "Epoch 358/1000, Loss: 0.08755047619342804\n",
      "Epoch 359/1000, Loss: 0.08755530416965485\n",
      "Epoch 360/1000, Loss: 0.08755040168762207\n",
      "Epoch 361/1000, Loss: 0.08754783868789673\n",
      "Epoch 362/1000, Loss: 0.08754333108663559\n",
      "Epoch 363/1000, Loss: 0.08753875643014908\n",
      "Epoch 364/1000, Loss: 0.08753697574138641\n",
      "Epoch 365/1000, Loss: 0.0875338762998581\n",
      "Epoch 366/1000, Loss: 0.08753447234630585\n",
      "Epoch 367/1000, Loss: 0.08753913640975952\n",
      "Epoch 368/1000, Loss: 0.08753013610839844\n",
      "Epoch 369/1000, Loss: 0.08753101527690887\n",
      "Epoch 370/1000, Loss: 0.08753100782632828\n",
      "Epoch 371/1000, Loss: 0.08753320574760437\n",
      "Epoch 372/1000, Loss: 0.08753890544176102\n",
      "Epoch 373/1000, Loss: 0.08754535019397736\n",
      "Epoch 374/1000, Loss: 0.08756017684936523\n",
      "Epoch 375/1000, Loss: 0.08754444122314453\n",
      "Epoch 376/1000, Loss: 0.08753011375665665\n",
      "Epoch 377/1000, Loss: 0.08752022683620453\n",
      "Epoch 378/1000, Loss: 0.08752211183309555\n",
      "Epoch 379/1000, Loss: 0.08753141760826111\n",
      "Epoch 380/1000, Loss: 0.08752796053886414\n",
      "Epoch 381/1000, Loss: 0.0875333845615387\n",
      "Epoch 382/1000, Loss: 0.08753260225057602\n",
      "Epoch 383/1000, Loss: 0.08753124624490738\n",
      "Epoch 384/1000, Loss: 0.08751921355724335\n",
      "Epoch 385/1000, Loss: 0.08750981837511063\n",
      "Epoch 386/1000, Loss: 0.08750846236944199\n",
      "Epoch 387/1000, Loss: 0.08751480281352997\n",
      "Epoch 388/1000, Loss: 0.08752111345529556\n",
      "Epoch 389/1000, Loss: 0.08752284198999405\n",
      "Epoch 390/1000, Loss: 0.08752018958330154\n",
      "Epoch 391/1000, Loss: 0.0875144824385643\n",
      "Epoch 392/1000, Loss: 0.0875195562839508\n",
      "Epoch 393/1000, Loss: 0.08751902729272842\n",
      "Epoch 394/1000, Loss: 0.08750924468040466\n",
      "Epoch 395/1000, Loss: 0.08750101178884506\n",
      "Epoch 396/1000, Loss: 0.08750094473361969\n",
      "Epoch 397/1000, Loss: 0.08750834316015244\n",
      "Epoch 398/1000, Loss: 0.08751580119132996\n",
      "Epoch 399/1000, Loss: 0.08751554042100906\n",
      "Epoch 400/1000, Loss: 0.08750477433204651\n",
      "Epoch 401/1000, Loss: 0.08750120550394058\n",
      "Epoch 402/1000, Loss: 0.08750727772712708\n",
      "Epoch 403/1000, Loss: 0.08751006424427032\n",
      "Epoch 404/1000, Loss: 0.08750451356172562\n",
      "Epoch 405/1000, Loss: 0.08750811964273453\n",
      "Epoch 406/1000, Loss: 0.0875055119395256\n",
      "Epoch 407/1000, Loss: 0.08750613033771515\n",
      "Epoch 408/1000, Loss: 0.08749707788228989\n",
      "Epoch 409/1000, Loss: 0.08748896420001984\n",
      "Epoch 410/1000, Loss: 0.08749022334814072\n",
      "Epoch 411/1000, Loss: 0.08749735355377197\n",
      "Epoch 412/1000, Loss: 0.08749982714653015\n",
      "Epoch 413/1000, Loss: 0.08749603480100632\n",
      "Epoch 414/1000, Loss: 0.0874895229935646\n",
      "Epoch 415/1000, Loss: 0.08748447895050049\n",
      "Epoch 416/1000, Loss: 0.0874880775809288\n",
      "Epoch 417/1000, Loss: 0.08749766647815704\n",
      "Epoch 418/1000, Loss: 0.08750682324171066\n",
      "Epoch 419/1000, Loss: 0.0875198245048523\n",
      "Epoch 420/1000, Loss: 0.08754701167345047\n",
      "Epoch 421/1000, Loss: 0.08752353489398956\n",
      "Epoch 422/1000, Loss: 0.08749149739742279\n",
      "Epoch 423/1000, Loss: 0.08748430758714676\n",
      "Epoch 424/1000, Loss: 0.08750202506780624\n",
      "Epoch 425/1000, Loss: 0.08752621710300446\n",
      "Epoch 426/1000, Loss: 0.0874907597899437\n",
      "Epoch 427/1000, Loss: 0.08748501539230347\n",
      "Epoch 428/1000, Loss: 0.08749821037054062\n",
      "Epoch 429/1000, Loss: 0.0874939113855362\n",
      "Epoch 430/1000, Loss: 0.08748503774404526\n",
      "Epoch 431/1000, Loss: 0.08749210089445114\n",
      "Epoch 432/1000, Loss: 0.08748556673526764\n",
      "Epoch 433/1000, Loss: 0.08747280389070511\n",
      "Epoch 434/1000, Loss: 0.08748044073581696\n",
      "Epoch 435/1000, Loss: 0.08749157190322876\n",
      "Epoch 436/1000, Loss: 0.08748254925012589\n",
      "Epoch 437/1000, Loss: 0.08747025579214096\n",
      "Epoch 438/1000, Loss: 0.08747892826795578\n",
      "Epoch 439/1000, Loss: 0.08748319000005722\n",
      "Epoch 440/1000, Loss: 0.08747989684343338\n",
      "Epoch 441/1000, Loss: 0.08747459203004837\n",
      "Epoch 442/1000, Loss: 0.0874849259853363\n",
      "Epoch 443/1000, Loss: 0.08748501539230347\n",
      "Epoch 444/1000, Loss: 0.08747056871652603\n",
      "Epoch 445/1000, Loss: 0.08746315538883209\n",
      "Epoch 446/1000, Loss: 0.08747018128633499\n",
      "Epoch 447/1000, Loss: 0.08748072385787964\n",
      "Epoch 448/1000, Loss: 0.08747056126594543\n",
      "Epoch 449/1000, Loss: 0.0874713584780693\n",
      "Epoch 450/1000, Loss: 0.08747455477714539\n",
      "Epoch 451/1000, Loss: 0.08746933937072754\n",
      "Epoch 452/1000, Loss: 0.08745912462472916\n",
      "Epoch 453/1000, Loss: 0.08745601773262024\n",
      "Epoch 454/1000, Loss: 0.08746093511581421\n",
      "Epoch 455/1000, Loss: 0.08746177703142166\n",
      "Epoch 456/1000, Loss: 0.08746049553155899\n",
      "Epoch 457/1000, Loss: 0.087465301156044\n",
      "Epoch 458/1000, Loss: 0.08749017119407654\n",
      "Epoch 459/1000, Loss: 0.08748523890972137\n",
      "Epoch 460/1000, Loss: 0.08748387545347214\n",
      "Epoch 461/1000, Loss: 0.08747012168169022\n",
      "Epoch 462/1000, Loss: 0.08746485412120819\n",
      "Epoch 463/1000, Loss: 0.0874648243188858\n",
      "Epoch 464/1000, Loss: 0.0874701589345932\n",
      "Epoch 465/1000, Loss: 0.08747166395187378\n",
      "Epoch 466/1000, Loss: 0.0874623954296112\n",
      "Epoch 467/1000, Loss: 0.08745762705802917\n",
      "Epoch 468/1000, Loss: 0.08746080100536346\n",
      "Epoch 469/1000, Loss: 0.0874687135219574\n",
      "Epoch 470/1000, Loss: 0.08746697008609772\n",
      "Epoch 471/1000, Loss: 0.08745989948511124\n",
      "Epoch 472/1000, Loss: 0.08745092898607254\n",
      "Epoch 473/1000, Loss: 0.08744857460260391\n",
      "Epoch 474/1000, Loss: 0.08746124804019928\n",
      "Epoch 475/1000, Loss: 0.08746109902858734\n",
      "Epoch 476/1000, Loss: 0.08746041357517242\n",
      "Epoch 477/1000, Loss: 0.08745522797107697\n",
      "Epoch 478/1000, Loss: 0.08744842559099197\n",
      "Epoch 479/1000, Loss: 0.0874471440911293\n",
      "Epoch 480/1000, Loss: 0.08744942396879196\n",
      "Epoch 481/1000, Loss: 0.08745210617780685\n",
      "Epoch 482/1000, Loss: 0.08744800090789795\n",
      "Epoch 483/1000, Loss: 0.08744287490844727\n",
      "Epoch 484/1000, Loss: 0.08744348585605621\n",
      "Epoch 485/1000, Loss: 0.08744842559099197\n",
      "Epoch 486/1000, Loss: 0.08746225386857986\n",
      "Epoch 487/1000, Loss: 0.08746960759162903\n",
      "Epoch 488/1000, Loss: 0.08748556673526764\n",
      "Epoch 489/1000, Loss: 0.08746771514415741\n",
      "Epoch 490/1000, Loss: 0.08745814859867096\n",
      "Epoch 491/1000, Loss: 0.08744301646947861\n",
      "Epoch 492/1000, Loss: 0.08744371682405472\n",
      "Epoch 493/1000, Loss: 0.08745156973600388\n",
      "Epoch 494/1000, Loss: 0.08745253831148148\n",
      "Epoch 495/1000, Loss: 0.08745253831148148\n",
      "Epoch 496/1000, Loss: 0.08744741231203079\n",
      "Epoch 497/1000, Loss: 0.0874452143907547\n",
      "Epoch 498/1000, Loss: 0.0874420776963234\n",
      "Epoch 499/1000, Loss: 0.08743738383054733\n",
      "Epoch 500/1000, Loss: 0.0874430239200592\n",
      "Epoch 501/1000, Loss: 0.087449811398983\n",
      "Epoch 502/1000, Loss: 0.08745971322059631\n",
      "Epoch 503/1000, Loss: 0.08744088560342789\n",
      "Epoch 504/1000, Loss: 0.08743247389793396\n",
      "Epoch 505/1000, Loss: 0.08743671327829361\n",
      "Epoch 506/1000, Loss: 0.08744105696678162\n",
      "Epoch 507/1000, Loss: 0.08743725717067719\n",
      "Epoch 508/1000, Loss: 0.08742853254079819\n",
      "Epoch 509/1000, Loss: 0.08742910623550415\n",
      "Epoch 510/1000, Loss: 0.08743669092655182\n",
      "Epoch 511/1000, Loss: 0.08743662387132645\n",
      "Epoch 512/1000, Loss: 0.08743161708116531\n",
      "Epoch 513/1000, Loss: 0.08742886781692505\n",
      "Epoch 514/1000, Loss: 0.08743226528167725\n",
      "Epoch 515/1000, Loss: 0.08743544667959213\n",
      "Epoch 516/1000, Loss: 0.08743009716272354\n",
      "Epoch 517/1000, Loss: 0.08742588758468628\n",
      "Epoch 518/1000, Loss: 0.08742611855268478\n",
      "Epoch 519/1000, Loss: 0.08742797374725342\n",
      "Epoch 520/1000, Loss: 0.08742580562829971\n",
      "Epoch 521/1000, Loss: 0.0874238908290863\n",
      "Epoch 522/1000, Loss: 0.08742297440767288\n",
      "Epoch 523/1000, Loss: 0.08742646872997284\n",
      "Epoch 524/1000, Loss: 0.08742394298315048\n",
      "Epoch 525/1000, Loss: 0.08742619305849075\n",
      "Epoch 526/1000, Loss: 0.08742596954107285\n",
      "Epoch 527/1000, Loss: 0.08743636310100555\n",
      "Epoch 528/1000, Loss: 0.08744741231203079\n",
      "Epoch 529/1000, Loss: 0.08747098594903946\n",
      "Epoch 530/1000, Loss: 0.08746182173490524\n",
      "Epoch 531/1000, Loss: 0.08743850886821747\n",
      "Epoch 532/1000, Loss: 0.08741874247789383\n",
      "Epoch 533/1000, Loss: 0.08742547035217285\n",
      "Epoch 534/1000, Loss: 0.08743692934513092\n",
      "Epoch 535/1000, Loss: 0.08744674175977707\n",
      "Epoch 536/1000, Loss: 0.08745236694812775\n",
      "Epoch 537/1000, Loss: 0.08742595463991165\n",
      "Epoch 538/1000, Loss: 0.08741740137338638\n",
      "Epoch 539/1000, Loss: 0.08742150664329529\n",
      "Epoch 540/1000, Loss: 0.08742626756429672\n",
      "Epoch 541/1000, Loss: 0.08742772042751312\n",
      "Epoch 542/1000, Loss: 0.08742013573646545\n",
      "Epoch 543/1000, Loss: 0.0874166190624237\n",
      "Epoch 544/1000, Loss: 0.08741039782762527\n",
      "Epoch 545/1000, Loss: 0.08741214126348495\n",
      "Epoch 546/1000, Loss: 0.08742016553878784\n",
      "Epoch 547/1000, Loss: 0.08741865307092667\n",
      "Epoch 548/1000, Loss: 0.08741524815559387\n",
      "Epoch 549/1000, Loss: 0.0874091386795044\n",
      "Epoch 550/1000, Loss: 0.08741221576929092\n",
      "Epoch 551/1000, Loss: 0.08741365373134613\n",
      "Epoch 552/1000, Loss: 0.08740866184234619\n",
      "Epoch 553/1000, Loss: 0.08740834891796112\n",
      "Epoch 554/1000, Loss: 0.0874069333076477\n",
      "Epoch 555/1000, Loss: 0.08740808814764023\n",
      "Epoch 556/1000, Loss: 0.08740958571434021\n",
      "Epoch 557/1000, Loss: 0.08740923553705215\n",
      "Epoch 558/1000, Loss: 0.08741314709186554\n",
      "Epoch 559/1000, Loss: 0.08741742372512817\n",
      "Epoch 560/1000, Loss: 0.0874161422252655\n",
      "Epoch 561/1000, Loss: 0.08741414546966553\n",
      "Epoch 562/1000, Loss: 0.08741197735071182\n",
      "Epoch 563/1000, Loss: 0.08740222454071045\n",
      "Epoch 564/1000, Loss: 0.08740320056676865\n",
      "Epoch 565/1000, Loss: 0.08740026503801346\n",
      "Epoch 566/1000, Loss: 0.08740036189556122\n",
      "Epoch 567/1000, Loss: 0.08739955723285675\n",
      "Epoch 568/1000, Loss: 0.08739937841892242\n",
      "Epoch 569/1000, Loss: 0.08740770071744919\n",
      "Epoch 570/1000, Loss: 0.08741655200719833\n",
      "Epoch 571/1000, Loss: 0.08743226528167725\n",
      "Epoch 572/1000, Loss: 0.08744087815284729\n",
      "Epoch 573/1000, Loss: 0.08744747191667557\n",
      "Epoch 574/1000, Loss: 0.08742491155862808\n",
      "Epoch 575/1000, Loss: 0.08742064237594604\n",
      "Epoch 576/1000, Loss: 0.0874185711145401\n",
      "Epoch 577/1000, Loss: 0.08741318434476852\n",
      "Epoch 578/1000, Loss: 0.08740905672311783\n",
      "Epoch 579/1000, Loss: 0.08741093426942825\n",
      "Epoch 580/1000, Loss: 0.08741242438554764\n",
      "Epoch 581/1000, Loss: 0.0874115377664566\n",
      "Epoch 582/1000, Loss: 0.08741006255149841\n",
      "Epoch 583/1000, Loss: 0.08739462494850159\n",
      "Epoch 584/1000, Loss: 0.08740643411874771\n",
      "Epoch 585/1000, Loss: 0.08740793168544769\n",
      "Epoch 586/1000, Loss: 0.08740616589784622\n",
      "Epoch 587/1000, Loss: 0.08740220218896866\n",
      "Epoch 588/1000, Loss: 0.087398000061512\n",
      "Epoch 589/1000, Loss: 0.08740134537220001\n",
      "Epoch 590/1000, Loss: 0.08739447593688965\n",
      "Epoch 591/1000, Loss: 0.08740225434303284\n",
      "Epoch 592/1000, Loss: 0.0873928815126419\n",
      "Epoch 593/1000, Loss: 0.08739214390516281\n",
      "Epoch 594/1000, Loss: 0.08739334344863892\n",
      "Epoch 595/1000, Loss: 0.08739403635263443\n",
      "Epoch 596/1000, Loss: 0.08739572018384933\n",
      "Epoch 597/1000, Loss: 0.08739272505044937\n",
      "Epoch 598/1000, Loss: 0.08739189803600311\n",
      "Epoch 599/1000, Loss: 0.08738746494054794\n",
      "Epoch 600/1000, Loss: 0.08739037066698074\n",
      "Epoch 601/1000, Loss: 0.08739429712295532\n",
      "Epoch 602/1000, Loss: 0.08739574998617172\n",
      "Epoch 603/1000, Loss: 0.08739832043647766\n",
      "Epoch 604/1000, Loss: 0.08740242570638657\n",
      "Epoch 605/1000, Loss: 0.08740010857582092\n",
      "Epoch 606/1000, Loss: 0.08739275485277176\n",
      "Epoch 607/1000, Loss: 0.08739311993122101\n",
      "Epoch 608/1000, Loss: 0.08739493042230606\n",
      "Epoch 609/1000, Loss: 0.08739722520112991\n",
      "Epoch 610/1000, Loss: 0.08739817142486572\n",
      "Epoch 611/1000, Loss: 0.0874091312289238\n",
      "Epoch 612/1000, Loss: 0.08742048591375351\n",
      "Epoch 613/1000, Loss: 0.08743748068809509\n",
      "Epoch 614/1000, Loss: 0.08742831647396088\n",
      "Epoch 615/1000, Loss: 0.08739923685789108\n",
      "Epoch 616/1000, Loss: 0.08739043027162552\n",
      "Epoch 617/1000, Loss: 0.08740636706352234\n",
      "Epoch 618/1000, Loss: 0.08741289377212524\n",
      "Epoch 619/1000, Loss: 0.08740229159593582\n",
      "Epoch 620/1000, Loss: 0.08740599453449249\n",
      "Epoch 621/1000, Loss: 0.08740018308162689\n",
      "Epoch 622/1000, Loss: 0.08739078789949417\n",
      "Epoch 623/1000, Loss: 0.08738270401954651\n",
      "Epoch 624/1000, Loss: 0.08738750964403152\n",
      "Epoch 625/1000, Loss: 0.08739802241325378\n",
      "Epoch 626/1000, Loss: 0.0874006375670433\n",
      "Epoch 627/1000, Loss: 0.08739986270666122\n",
      "Epoch 628/1000, Loss: 0.08738326281309128\n",
      "Epoch 629/1000, Loss: 0.08738122135400772\n",
      "Epoch 630/1000, Loss: 0.08738937973976135\n",
      "Epoch 631/1000, Loss: 0.08738000690937042\n",
      "Epoch 632/1000, Loss: 0.08738557994365692\n",
      "Epoch 633/1000, Loss: 0.08738210797309875\n",
      "Epoch 634/1000, Loss: 0.08738896995782852\n",
      "Epoch 635/1000, Loss: 0.08738605678081512\n",
      "Epoch 636/1000, Loss: 0.08738705515861511\n",
      "Epoch 637/1000, Loss: 0.08737633377313614\n",
      "Epoch 638/1000, Loss: 0.0873701274394989\n",
      "Epoch 639/1000, Loss: 0.08737150579690933\n",
      "Epoch 640/1000, Loss: 0.08737953752279282\n",
      "Epoch 641/1000, Loss: 0.08739335089921951\n",
      "Epoch 642/1000, Loss: 0.08739358186721802\n",
      "Epoch 643/1000, Loss: 0.08739177137613297\n",
      "Epoch 644/1000, Loss: 0.0873793289065361\n",
      "Epoch 645/1000, Loss: 0.08738166838884354\n",
      "Epoch 646/1000, Loss: 0.08737485110759735\n",
      "Epoch 647/1000, Loss: 0.0873710960149765\n",
      "Epoch 648/1000, Loss: 0.08737873286008835\n",
      "Epoch 649/1000, Loss: 0.08737805485725403\n",
      "Epoch 650/1000, Loss: 0.0873807743191719\n",
      "Epoch 651/1000, Loss: 0.08737436681985855\n",
      "Epoch 652/1000, Loss: 0.08738083392381668\n",
      "Epoch 653/1000, Loss: 0.08737476170063019\n",
      "Epoch 654/1000, Loss: 0.08737348765134811\n",
      "Epoch 655/1000, Loss: 0.08736887574195862\n",
      "Epoch 656/1000, Loss: 0.08737432956695557\n",
      "Epoch 657/1000, Loss: 0.08736875653266907\n",
      "Epoch 658/1000, Loss: 0.08736594021320343\n",
      "Epoch 659/1000, Loss: 0.0873713418841362\n",
      "Epoch 660/1000, Loss: 0.08738027513027191\n",
      "Epoch 661/1000, Loss: 0.0873812809586525\n",
      "Epoch 662/1000, Loss: 0.08738728612661362\n",
      "Epoch 663/1000, Loss: 0.0873730406165123\n",
      "Epoch 664/1000, Loss: 0.08736936748027802\n",
      "Epoch 665/1000, Loss: 0.08736453950405121\n",
      "Epoch 666/1000, Loss: 0.08737247437238693\n",
      "Epoch 667/1000, Loss: 0.08737512677907944\n",
      "Epoch 668/1000, Loss: 0.08736985176801682\n",
      "Epoch 669/1000, Loss: 0.08736608922481537\n",
      "Epoch 670/1000, Loss: 0.08736966550350189\n",
      "Epoch 671/1000, Loss: 0.08739671856164932\n",
      "Epoch 672/1000, Loss: 0.0873957946896553\n",
      "Epoch 673/1000, Loss: 0.08737937361001968\n",
      "Epoch 674/1000, Loss: 0.08735654503107071\n",
      "Epoch 675/1000, Loss: 0.08737374097108841\n",
      "Epoch 676/1000, Loss: 0.08739510178565979\n",
      "Epoch 677/1000, Loss: 0.08738234639167786\n",
      "Epoch 678/1000, Loss: 0.08736101537942886\n",
      "Epoch 679/1000, Loss: 0.0873662531375885\n",
      "Epoch 680/1000, Loss: 0.08738905191421509\n",
      "Epoch 681/1000, Loss: 0.08736637979745865\n",
      "Epoch 682/1000, Loss: 0.08735749870538712\n",
      "Epoch 683/1000, Loss: 0.08737383782863617\n",
      "Epoch 684/1000, Loss: 0.08738100528717041\n",
      "Epoch 685/1000, Loss: 0.08736933767795563\n",
      "Epoch 686/1000, Loss: 0.0873611643910408\n",
      "Epoch 687/1000, Loss: 0.08737590909004211\n",
      "Epoch 688/1000, Loss: 0.08738533407449722\n",
      "Epoch 689/1000, Loss: 0.08737073093652725\n",
      "Epoch 690/1000, Loss: 0.08735596388578415\n",
      "Epoch 691/1000, Loss: 0.08736952394247055\n",
      "Epoch 692/1000, Loss: 0.08736132085323334\n",
      "Epoch 693/1000, Loss: 0.0873526781797409\n",
      "Epoch 694/1000, Loss: 0.08735835552215576\n",
      "Epoch 695/1000, Loss: 0.08737039566040039\n",
      "Epoch 696/1000, Loss: 0.08737873286008835\n",
      "Epoch 697/1000, Loss: 0.0873664990067482\n",
      "Epoch 698/1000, Loss: 0.0873752310872078\n",
      "Epoch 699/1000, Loss: 0.08738410472869873\n",
      "Epoch 700/1000, Loss: 0.08737186342477798\n",
      "Epoch 701/1000, Loss: 0.08735596388578415\n",
      "Epoch 702/1000, Loss: 0.08736389875411987\n",
      "Epoch 703/1000, Loss: 0.08736217767000198\n",
      "Epoch 704/1000, Loss: 0.0873507484793663\n",
      "Epoch 705/1000, Loss: 0.08734917640686035\n",
      "Epoch 706/1000, Loss: 0.08735281229019165\n",
      "Epoch 707/1000, Loss: 0.08735672384500504\n",
      "Epoch 708/1000, Loss: 0.0873526930809021\n",
      "Epoch 709/1000, Loss: 0.08735296130180359\n",
      "Epoch 710/1000, Loss: 0.08735468983650208\n",
      "Epoch 711/1000, Loss: 0.08735990524291992\n",
      "Epoch 712/1000, Loss: 0.08735580742359161\n",
      "Epoch 713/1000, Loss: 0.08734887093305588\n",
      "Epoch 714/1000, Loss: 0.08735015243291855\n",
      "Epoch 715/1000, Loss: 0.08736100047826767\n",
      "Epoch 716/1000, Loss: 0.08734004944562912\n",
      "Epoch 717/1000, Loss: 0.08734358847141266\n",
      "Epoch 718/1000, Loss: 0.08735046535730362\n",
      "Epoch 719/1000, Loss: 0.0873509868979454\n",
      "Epoch 720/1000, Loss: 0.0873481035232544\n",
      "Epoch 721/1000, Loss: 0.08735835552215576\n",
      "Epoch 722/1000, Loss: 0.08736754208803177\n",
      "Epoch 723/1000, Loss: 0.08737645298242569\n",
      "Epoch 724/1000, Loss: 0.08739956468343735\n",
      "Epoch 725/1000, Loss: 0.0873887911438942\n",
      "Epoch 726/1000, Loss: 0.0873614102602005\n",
      "Epoch 727/1000, Loss: 0.08734304457902908\n",
      "Epoch 728/1000, Loss: 0.08735739439725876\n",
      "Epoch 729/1000, Loss: 0.08737950026988983\n",
      "Epoch 730/1000, Loss: 0.08737126737833023\n",
      "Epoch 731/1000, Loss: 0.08736194670200348\n",
      "Epoch 732/1000, Loss: 0.08734507113695145\n",
      "Epoch 733/1000, Loss: 0.08734562993049622\n",
      "Epoch 734/1000, Loss: 0.08736184984445572\n",
      "Epoch 735/1000, Loss: 0.08735551685094833\n",
      "Epoch 736/1000, Loss: 0.08734817057847977\n",
      "Epoch 737/1000, Loss: 0.08735422044992447\n",
      "Epoch 738/1000, Loss: 0.087350033223629\n",
      "Epoch 739/1000, Loss: 0.08734411001205444\n",
      "Epoch 740/1000, Loss: 0.0873517170548439\n",
      "Epoch 741/1000, Loss: 0.08734657615423203\n",
      "Epoch 742/1000, Loss: 0.08734608441591263\n",
      "Epoch 743/1000, Loss: 0.08735134452581406\n",
      "Epoch 744/1000, Loss: 0.08735396713018417\n",
      "Epoch 745/1000, Loss: 0.08735445886850357\n",
      "Epoch 746/1000, Loss: 0.08734551817178726\n",
      "Epoch 747/1000, Loss: 0.08734694868326187\n",
      "Epoch 748/1000, Loss: 0.08735350519418716\n",
      "Epoch 749/1000, Loss: 0.08735517412424088\n",
      "Epoch 750/1000, Loss: 0.08735614269971848\n",
      "Epoch 751/1000, Loss: 0.08734212815761566\n",
      "Epoch 752/1000, Loss: 0.08734601736068726\n",
      "Epoch 753/1000, Loss: 0.0873534306883812\n",
      "Epoch 754/1000, Loss: 0.08735111355781555\n",
      "Epoch 755/1000, Loss: 0.0873476192355156\n",
      "Epoch 756/1000, Loss: 0.08733939379453659\n",
      "Epoch 757/1000, Loss: 0.08732876181602478\n",
      "Epoch 758/1000, Loss: 0.08734124153852463\n",
      "Epoch 759/1000, Loss: 0.08734836429357529\n",
      "Epoch 760/1000, Loss: 0.08734030276536942\n",
      "Epoch 761/1000, Loss: 0.08734171837568283\n",
      "Epoch 762/1000, Loss: 0.08733484894037247\n",
      "Epoch 763/1000, Loss: 0.08733434230089188\n",
      "Epoch 764/1000, Loss: 0.08734658360481262\n",
      "Epoch 765/1000, Loss: 0.08735685050487518\n",
      "Epoch 766/1000, Loss: 0.08735855668783188\n",
      "Epoch 767/1000, Loss: 0.08737202733755112\n",
      "Epoch 768/1000, Loss: 0.08735717833042145\n",
      "Epoch 769/1000, Loss: 0.0873534306883812\n",
      "Epoch 770/1000, Loss: 0.08733882755041122\n",
      "Epoch 771/1000, Loss: 0.087344691157341\n",
      "Epoch 772/1000, Loss: 0.08735252916812897\n",
      "Epoch 773/1000, Loss: 0.08736646175384521\n",
      "Epoch 774/1000, Loss: 0.08737754076719284\n",
      "Epoch 775/1000, Loss: 0.08734861016273499\n",
      "Epoch 776/1000, Loss: 0.08732695132493973\n",
      "Epoch 777/1000, Loss: 0.08734152466058731\n",
      "Epoch 778/1000, Loss: 0.08734802901744843\n",
      "Epoch 779/1000, Loss: 0.08734779059886932\n",
      "Epoch 780/1000, Loss: 0.08733218163251877\n",
      "Epoch 781/1000, Loss: 0.08732801675796509\n",
      "Epoch 782/1000, Loss: 0.08733540028333664\n",
      "Epoch 783/1000, Loss: 0.08733267337083817\n",
      "Epoch 784/1000, Loss: 0.08733301609754562\n",
      "Epoch 785/1000, Loss: 0.08733583986759186\n",
      "Epoch 786/1000, Loss: 0.08733610063791275\n",
      "Epoch 787/1000, Loss: 0.08733639121055603\n",
      "Epoch 788/1000, Loss: 0.08733410388231277\n",
      "Epoch 789/1000, Loss: 0.08732504397630692\n",
      "Epoch 790/1000, Loss: 0.0873294398188591\n",
      "Epoch 791/1000, Loss: 0.0873289555311203\n",
      "Epoch 792/1000, Loss: 0.08732582628726959\n",
      "Epoch 793/1000, Loss: 0.08732769638299942\n",
      "Epoch 794/1000, Loss: 0.0873231291770935\n",
      "Epoch 795/1000, Loss: 0.08732110261917114\n",
      "Epoch 796/1000, Loss: 0.08732638508081436\n",
      "Epoch 797/1000, Loss: 0.08732905983924866\n",
      "Epoch 798/1000, Loss: 0.08732914924621582\n",
      "Epoch 799/1000, Loss: 0.08732949197292328\n",
      "Epoch 800/1000, Loss: 0.08731985837221146\n",
      "Epoch 801/1000, Loss: 0.08731475472450256\n",
      "Epoch 802/1000, Loss: 0.08731785416603088\n",
      "Epoch 803/1000, Loss: 0.08732002973556519\n",
      "Epoch 804/1000, Loss: 0.087320975959301\n",
      "Epoch 805/1000, Loss: 0.08731814473867416\n",
      "Epoch 806/1000, Loss: 0.08731574565172195\n",
      "Epoch 807/1000, Loss: 0.0873221606016159\n",
      "Epoch 808/1000, Loss: 0.0873303934931755\n",
      "Epoch 809/1000, Loss: 0.08734287321567535\n",
      "Epoch 810/1000, Loss: 0.08738292008638382\n",
      "Epoch 811/1000, Loss: 0.0873817726969719\n",
      "Epoch 812/1000, Loss: 0.08736857026815414\n",
      "Epoch 813/1000, Loss: 0.08733348548412323\n",
      "Epoch 814/1000, Loss: 0.08733856678009033\n",
      "Epoch 815/1000, Loss: 0.08737099915742874\n",
      "Epoch 816/1000, Loss: 0.08736629039049149\n",
      "Epoch 817/1000, Loss: 0.08734498172998428\n",
      "Epoch 818/1000, Loss: 0.08733432739973068\n",
      "Epoch 819/1000, Loss: 0.0873527079820633\n",
      "Epoch 820/1000, Loss: 0.08735174685716629\n",
      "Epoch 821/1000, Loss: 0.08734233677387238\n",
      "Epoch 822/1000, Loss: 0.08734215050935745\n",
      "Epoch 823/1000, Loss: 0.08733714371919632\n",
      "Epoch 824/1000, Loss: 0.08734185993671417\n",
      "Epoch 825/1000, Loss: 0.0873347595334053\n",
      "Epoch 826/1000, Loss: 0.08733174949884415\n",
      "Epoch 827/1000, Loss: 0.08733075112104416\n",
      "Epoch 828/1000, Loss: 0.0873263031244278\n",
      "Epoch 829/1000, Loss: 0.08732814341783524\n",
      "Epoch 830/1000, Loss: 0.0873437225818634\n",
      "Epoch 831/1000, Loss: 0.08731870353221893\n",
      "Epoch 832/1000, Loss: 0.0873236283659935\n",
      "Epoch 833/1000, Loss: 0.08733497560024261\n",
      "Epoch 834/1000, Loss: 0.08732529729604721\n",
      "Epoch 835/1000, Loss: 0.08732591569423676\n",
      "Epoch 836/1000, Loss: 0.0873238667845726\n",
      "Epoch 837/1000, Loss: 0.08731112629175186\n",
      "Epoch 838/1000, Loss: 0.08731218427419662\n",
      "Epoch 839/1000, Loss: 0.08731093257665634\n",
      "Epoch 840/1000, Loss: 0.08730771392583847\n",
      "Epoch 841/1000, Loss: 0.08731592446565628\n",
      "Epoch 842/1000, Loss: 0.08730490505695343\n",
      "Epoch 843/1000, Loss: 0.0873042568564415\n",
      "Epoch 844/1000, Loss: 0.08731132000684738\n",
      "Epoch 845/1000, Loss: 0.08730316907167435\n",
      "Epoch 846/1000, Loss: 0.08729825913906097\n",
      "Epoch 847/1000, Loss: 0.08729849755764008\n",
      "Epoch 848/1000, Loss: 0.08730048686265945\n",
      "Epoch 849/1000, Loss: 0.0873129591345787\n",
      "Epoch 850/1000, Loss: 0.08731723576784134\n",
      "Epoch 851/1000, Loss: 0.08733659982681274\n",
      "Epoch 852/1000, Loss: 0.08733417838811874\n",
      "Epoch 853/1000, Loss: 0.08734399825334549\n",
      "Epoch 854/1000, Loss: 0.08730916678905487\n",
      "Epoch 855/1000, Loss: 0.08730160444974899\n",
      "Epoch 856/1000, Loss: 0.08732406049966812\n",
      "Epoch 857/1000, Loss: 0.08733029663562775\n",
      "Epoch 858/1000, Loss: 0.0873163491487503\n",
      "Epoch 859/1000, Loss: 0.08730585873126984\n",
      "Epoch 860/1000, Loss: 0.08730735629796982\n",
      "Epoch 861/1000, Loss: 0.08730549365282059\n",
      "Epoch 862/1000, Loss: 0.08730407059192657\n",
      "Epoch 863/1000, Loss: 0.08731430768966675\n",
      "Epoch 864/1000, Loss: 0.0873037725687027\n",
      "Epoch 865/1000, Loss: 0.08730720728635788\n",
      "Epoch 866/1000, Loss: 0.08729317784309387\n",
      "Epoch 867/1000, Loss: 0.0873066708445549\n",
      "Epoch 868/1000, Loss: 0.08730912953615189\n",
      "Epoch 869/1000, Loss: 0.08729979395866394\n",
      "Epoch 870/1000, Loss: 0.0873008444905281\n",
      "Epoch 871/1000, Loss: 0.08730214834213257\n",
      "Epoch 872/1000, Loss: 0.08729550242424011\n",
      "Epoch 873/1000, Loss: 0.08728475123643875\n",
      "Epoch 874/1000, Loss: 0.08729495853185654\n",
      "Epoch 875/1000, Loss: 0.08730091899633408\n",
      "Epoch 876/1000, Loss: 0.08730004727840424\n",
      "Epoch 877/1000, Loss: 0.08731083571910858\n",
      "Epoch 878/1000, Loss: 0.08731193095445633\n",
      "Epoch 879/1000, Loss: 0.08732427656650543\n",
      "Epoch 880/1000, Loss: 0.08730080723762512\n",
      "Epoch 881/1000, Loss: 0.08729851990938187\n",
      "Epoch 882/1000, Loss: 0.08729816228151321\n",
      "Epoch 883/1000, Loss: 0.08730821311473846\n",
      "Epoch 884/1000, Loss: 0.0873163565993309\n",
      "Epoch 885/1000, Loss: 0.08731228858232498\n",
      "Epoch 886/1000, Loss: 0.08731021732091904\n",
      "Epoch 887/1000, Loss: 0.08728957176208496\n",
      "Epoch 888/1000, Loss: 0.08729194104671478\n",
      "Epoch 889/1000, Loss: 0.08729714900255203\n",
      "Epoch 890/1000, Loss: 0.08731868863105774\n",
      "Epoch 891/1000, Loss: 0.08733196556568146\n",
      "Epoch 892/1000, Loss: 0.08729681372642517\n",
      "Epoch 893/1000, Loss: 0.08729416131973267\n",
      "Epoch 894/1000, Loss: 0.08729957044124603\n",
      "Epoch 895/1000, Loss: 0.0872979387640953\n",
      "Epoch 896/1000, Loss: 0.08730196952819824\n",
      "Epoch 897/1000, Loss: 0.08730525523424149\n",
      "Epoch 898/1000, Loss: 0.08729438483715057\n",
      "Epoch 899/1000, Loss: 0.08727876842021942\n",
      "Epoch 900/1000, Loss: 0.08728355169296265\n",
      "Epoch 901/1000, Loss: 0.08730607479810715\n",
      "Epoch 902/1000, Loss: 0.08730044960975647\n",
      "Epoch 903/1000, Loss: 0.0872911810874939\n",
      "Epoch 904/1000, Loss: 0.08728031069040298\n",
      "Epoch 905/1000, Loss: 0.08728329837322235\n",
      "Epoch 906/1000, Loss: 0.08728870004415512\n",
      "Epoch 907/1000, Loss: 0.08730233460664749\n",
      "Epoch 908/1000, Loss: 0.08730916678905487\n",
      "Epoch 909/1000, Loss: 0.08728998154401779\n",
      "Epoch 910/1000, Loss: 0.08727991580963135\n",
      "Epoch 911/1000, Loss: 0.08728624135255814\n",
      "Epoch 912/1000, Loss: 0.08729343116283417\n",
      "Epoch 913/1000, Loss: 0.08728697150945663\n",
      "Epoch 914/1000, Loss: 0.08728256821632385\n",
      "Epoch 915/1000, Loss: 0.08729007840156555\n",
      "Epoch 916/1000, Loss: 0.08728976547718048\n",
      "Epoch 917/1000, Loss: 0.08728335052728653\n",
      "Epoch 918/1000, Loss: 0.0872785747051239\n",
      "Epoch 919/1000, Loss: 0.08726941794157028\n",
      "Epoch 920/1000, Loss: 0.0872778668999672\n",
      "Epoch 921/1000, Loss: 0.08728082478046417\n",
      "Epoch 922/1000, Loss: 0.08727622777223587\n",
      "Epoch 923/1000, Loss: 0.08727294206619263\n",
      "Epoch 924/1000, Loss: 0.08727162331342697\n",
      "Epoch 925/1000, Loss: 0.08726433664560318\n",
      "Epoch 926/1000, Loss: 0.08727070689201355\n",
      "Epoch 927/1000, Loss: 0.0872659832239151\n",
      "Epoch 928/1000, Loss: 0.08726538717746735\n",
      "Epoch 929/1000, Loss: 0.08727575093507767\n",
      "Epoch 930/1000, Loss: 0.08727823942899704\n",
      "Epoch 931/1000, Loss: 0.08729279041290283\n",
      "Epoch 932/1000, Loss: 0.08730291575193405\n",
      "Epoch 933/1000, Loss: 0.08730894327163696\n",
      "Epoch 934/1000, Loss: 0.0872853547334671\n",
      "Epoch 935/1000, Loss: 0.08726891130208969\n",
      "Epoch 936/1000, Loss: 0.08726910501718521\n",
      "Epoch 937/1000, Loss: 0.08727959543466568\n",
      "Epoch 938/1000, Loss: 0.08730529248714447\n",
      "Epoch 939/1000, Loss: 0.08730337023735046\n",
      "Epoch 940/1000, Loss: 0.08729933947324753\n",
      "Epoch 941/1000, Loss: 0.08727312833070755\n",
      "Epoch 942/1000, Loss: 0.08728902041912079\n",
      "Epoch 943/1000, Loss: 0.0873011127114296\n",
      "Epoch 944/1000, Loss: 0.087288998067379\n",
      "Epoch 945/1000, Loss: 0.0872829332947731\n",
      "Epoch 946/1000, Loss: 0.08727893233299255\n",
      "Epoch 947/1000, Loss: 0.08729765564203262\n",
      "Epoch 948/1000, Loss: 0.08728362619876862\n",
      "Epoch 949/1000, Loss: 0.08727540075778961\n",
      "Epoch 950/1000, Loss: 0.08727234601974487\n",
      "Epoch 951/1000, Loss: 0.08727716654539108\n",
      "Epoch 952/1000, Loss: 0.08727708458900452\n",
      "Epoch 953/1000, Loss: 0.08726678043603897\n",
      "Epoch 954/1000, Loss: 0.08726991713047028\n",
      "Epoch 955/1000, Loss: 0.08726944029331207\n",
      "Epoch 956/1000, Loss: 0.08728335797786713\n",
      "Epoch 957/1000, Loss: 0.08730176836252213\n",
      "Epoch 958/1000, Loss: 0.08730213344097137\n",
      "Epoch 959/1000, Loss: 0.08727537840604782\n",
      "Epoch 960/1000, Loss: 0.08726150542497635\n",
      "Epoch 961/1000, Loss: 0.08726955950260162\n",
      "Epoch 962/1000, Loss: 0.08728297054767609\n",
      "Epoch 963/1000, Loss: 0.0872831866145134\n",
      "Epoch 964/1000, Loss: 0.0872950404882431\n",
      "Epoch 965/1000, Loss: 0.08729331940412521\n",
      "Epoch 966/1000, Loss: 0.08728198707103729\n",
      "Epoch 967/1000, Loss: 0.08726757019758224\n",
      "Epoch 968/1000, Loss: 0.08728311955928802\n",
      "Epoch 969/1000, Loss: 0.08731061965227127\n",
      "Epoch 970/1000, Loss: 0.08728674799203873\n",
      "Epoch 971/1000, Loss: 0.08728989958763123\n",
      "Epoch 972/1000, Loss: 0.08728576451539993\n",
      "Epoch 973/1000, Loss: 0.08726664632558823\n",
      "Epoch 974/1000, Loss: 0.08727304637432098\n",
      "Epoch 975/1000, Loss: 0.0872902199625969\n",
      "Epoch 976/1000, Loss: 0.08729485422372818\n",
      "Epoch 977/1000, Loss: 0.0872669517993927\n",
      "Epoch 978/1000, Loss: 0.08727895468473434\n",
      "Epoch 979/1000, Loss: 0.08727704733610153\n",
      "Epoch 980/1000, Loss: 0.08725958317518234\n",
      "Epoch 981/1000, Loss: 0.08726342767477036\n",
      "Epoch 982/1000, Loss: 0.08728720247745514\n",
      "Epoch 983/1000, Loss: 0.08728354424238205\n",
      "Epoch 984/1000, Loss: 0.08726455271244049\n",
      "Epoch 985/1000, Loss: 0.08726570010185242\n",
      "Epoch 986/1000, Loss: 0.08726347237825394\n",
      "Epoch 987/1000, Loss: 0.08726456761360168\n",
      "Epoch 988/1000, Loss: 0.08726460486650467\n",
      "Epoch 989/1000, Loss: 0.08728470653295517\n",
      "Epoch 990/1000, Loss: 0.08730489015579224\n",
      "Epoch 991/1000, Loss: 0.08728978782892227\n",
      "Epoch 992/1000, Loss: 0.08728189021348953\n",
      "Epoch 993/1000, Loss: 0.08726262301206589\n",
      "Epoch 994/1000, Loss: 0.0872652605175972\n",
      "Epoch 995/1000, Loss: 0.08730223029851913\n",
      "Epoch 996/1000, Loss: 0.08728842437267303\n",
      "Epoch 997/1000, Loss: 0.08726513385772705\n",
      "Epoch 998/1000, Loss: 0.08726672828197479\n",
      "Epoch 999/1000, Loss: 0.08729017525911331\n",
      "Epoch 1000/1000, Loss: 0.08727018535137177\n"
     ]
    }
   ],
   "source": [
    "print('x_split shape: ', x_split.shape)\n",
    "print('fc_matrices_dgbsl SHAPE: ', fc_matrices_dbgsl_tn.shape)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_episodes):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_split)\n",
    "    adjacency_matrix = construct_graph(outputs)\n",
    "    sparse_adjacency = sparsify(adjacency_matrix)\n",
    "    ground_truth = fc_matrices_dbgsl_tn.float()\n",
    "    loss = criterion(sparse_adjacency, ground_truth)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}/{n_episodes}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbgdgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
